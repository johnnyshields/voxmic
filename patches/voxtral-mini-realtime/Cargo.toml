[package]
name = "voxtral-mini-realtime"
version = "0.2.0"
edition = "2021"
description = "Voxtral Mini 4B Realtime - Streaming ASR in Rust with Burn"
license = "Apache-2.0"
repository = "https://github.com/TrevorS/voxtral-mini-realtime-rs"
authors = ["Trevor Strieber"]

[features]
default = ["wgpu", "native-tokenizer"]
wgpu = ["burn/wgpu"]
cli = ["clap", "indicatif"]
hub = ["hf-hub"]
# Native tokenizer (has C dependencies, not WASM compatible)
native-tokenizer = ["tokenizers"]
# WASM/browser support - uses wgpu (WebGPU) backend for GPU inference
wasm = [
    "wgpu",
    "dep:wgpu",
    "wasm-bindgen",
    "wasm-bindgen-futures",
    "js-sys",
    "web-sys",
    "console_error_panic_hook",
    "getrandom/wasm_js",
]
profiling = ["tracing-chrome"]

[dependencies]
# Burn ML framework - latest version with backend selection via features
burn = { version = "0.20", default-features = false, features = ["std", "autodiff", "template"] }
cubecl = { version = "0.9", default-features = false, features = ["wgpu"] }

# Tokenization (native only - has C dependencies)
tokenizers = { version = "0.22", optional = true }
base64 = "0.22"

# Audio processing
hound = "3.5"              # WAV I/O
rubato = "1.0"             # High-quality resampling
rustfft = "6.4"            # FFT for spectrograms
audioadapter-buffers = "2.0" # Buffer adapters for rubato

# Serialization & model loading
safetensors = "0.7"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Utilities
anyhow = "1.0"
thiserror = "2.0"
tracing = "0.1"
tracing-subscriber = "0.3"
tracing-chrome = { version = "0.7", optional = true }
num-traits = "0.2"
num-complex = "0.4"
byteorder = "1.5"
flate2 = "1.1"

# CLI (optional)
clap = { version = "4.5", features = ["derive"], optional = true }
indicatif = { version = "0.18", optional = true }

# HuggingFace Hub (optional)
hf-hub = { version = "0.4", default-features = false, features = ["ureq"], optional = true }
half = "2.7"

# NDArray for test data loading (used by test binary too)
ndarray = "0.16"
ndarray-npy = "0.9"

# WASM support (optional)
wasm-bindgen = { version = "0.2", optional = true }
wasm-bindgen-futures = { version = "0.4", optional = true }
js-sys = { version = "0.3", optional = true }
web-sys = { version = "0.3", features = ["console"], optional = true }
console_error_panic_hook = { version = "0.1", optional = true }
getrandom = { version = "0.3", default-features = false, optional = true }
# Direct wgpu dep for custom device init (must match cubecl-wgpu's version)
wgpu = { version = "26", features = ["fragile-send-sync-non-atomic-wasm"], optional = true }

[dev-dependencies]
criterion = "0.8"
approx = "0.5"
tempfile = "3.24"

[[bin]]
name = "voxtral-transcribe"
path = "src/bin/transcribe.rs"
required-features = ["cli"]

[[bin]]
name = "e2e-bench"
path = "src/bin/e2e_bench.rs"
required-features = ["wgpu", "cli"]

[[bench]]
name = "audio"
harness = false

[[bench]]
name = "q4_ops"
harness = false
required-features = ["wgpu"]

[[bench]]
name = "q4_pipeline"
harness = false
required-features = ["wgpu"]

# WASM library target
[lib]
crate-type = ["cdylib", "rlib"]

[profile.release]
lto = true
codegen-units = 1
opt-level = 3

[profile.release-wasm]
inherits = "release"
# Optimize for size in WASM
opt-level = "s"

[profile.profiling]
inherits = "release"
debug = 1

# Patch cubecl-wgpu to cap subgroup size workaround against WebGPU's
# max_compute_invocations_per_workgroup (256). Without this, cubek-reduce
# creates workgroups of 128*8=1024 which exceeds the WebGPU limit.
[patch.crates-io]
cubecl-wgpu = { path = "patches/cubecl-wgpu-0.9.0" }
